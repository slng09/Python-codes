{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slng09/effective-umbrella/blob/main/IOD_Demo_8_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El6Q2WYJqAjw"
      },
      "source": [
        "<div>\n",
        "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-Y1QLrXqAjz"
      },
      "source": [
        "# Demo 8.4: Stacking\n",
        "\n",
        "INSTRUCTIONS:\n",
        "\n",
        "- Run the cells\n",
        "- Observe and understand the results\n",
        "- Answer the questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye0DdBIEqAj2"
      },
      "source": [
        "This is an excerpt from the [Ensemble Learning to Improve Machine Learning Results-How ensemble methods work: bagging, boosting and stacking](https://blog.statsbot.co/ensemble-learning-d1dcd548e936) by **Vadim Smolyakov**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68MDqICRqAj4"
      },
      "source": [
        "## Stacking\n",
        "**Stacking** is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on complete training set then the meta-model is trained on the outputs of base level model as features. The base level often consists of different learning algorithms and therefore stacking ensembles are often heterogeneous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeeOjXkSqAj6"
      },
      "outputs": [],
      "source": [
        "## Import Libraries\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from mlxtend.classifier import StackingClassifier\n",
        "from mlxtend.plotting import plot_learning_curves\n",
        "from mlxtend.plotting import plot_decision_regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70k4lJOZqAkA"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKVMp6q5qAkB"
      },
      "outputs": [],
      "source": [
        "## Loading the dataset\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# picking just the first two features\n",
        "X = iris.data[:, 1:3]\n",
        "# target\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLZsVHN2qAkG"
      },
      "outputs": [],
      "source": [
        "## Check the data\n",
        "\n",
        "# About data\n",
        "print(X.shape)\n",
        "print(X[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcUA_dIiqAkM"
      },
      "outputs": [],
      "source": [
        "# About target\n",
        "print(y.shape)\n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnib-Ux8qAkR"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zviC8A9zqAkT"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "clf1 = KNeighborsClassifier(n_neighbors = 1)\n",
        "clf2 = RandomForestClassifier(n_estimators = 10, random_state = 1)\n",
        "clf3 = GaussianNB()\n",
        "lr = LogisticRegression(multi_class = 'auto', solver = 'lbfgs')\n",
        "sclf = StackingClassifier(\n",
        "    classifiers = [clf1, clf2, clf3],\n",
        "    meta_classifier = lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar_uU5ZuqAkW"
      },
      "source": [
        "## Presenting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmkT_objqAkX"
      },
      "outputs": [],
      "source": [
        "label = ['K-NN', 'Random Forest', 'Naïve Bayes', 'Stacking Classifier']\n",
        "clf_list = [clf1, clf2, clf3, sclf]\n",
        "    \n",
        "fig = plt.figure(figsize = (10, 8))\n",
        "gs = gridspec.GridSpec(2, 2)\n",
        "grid = itertools.product([0, 1], repeat = 2)\n",
        "\n",
        "clf_cv_mean = []\n",
        "clf_cv_std = []\n",
        "for clf, label, grd in zip(clf_list, label, grid):\n",
        "        \n",
        "    scores = cross_val_score(clf, X, y, cv = 3, scoring = 'accuracy')\n",
        "    print('Accuracy: %.2f (+/- %.2f) [%s]' % (scores.mean(), scores.std(), label))\n",
        "    clf_cv_mean.append(scores.mean())\n",
        "    clf_cv_std.append(scores.std())\n",
        "        \n",
        "    clf.fit(X, y)\n",
        "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
        "    fig = plot_decision_regions(X = X, y = y, clf = clf)\n",
        "    plt.title(label)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwqDtMdRqAkb"
      },
      "source": [
        "The stacking ensemble is illustrated int the figure above. It consists of k-NN, Random Forest and Naive Bayes base classifiers whose predictions are combined by Lostic Regression as a meta-classifier. We can see the blending of decision boundaries achieved by the stacking classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A34CQukFqAkd"
      },
      "outputs": [],
      "source": [
        "# plot learning curves\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 33)\n",
        "    \n",
        "plt.figure()\n",
        "plot_learning_curves(X_train, y_train, X_test, y_test, sclf, print_model = False, style = 'ggplot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0oyWvfiqAki"
      },
      "source": [
        "We can see that stacking achieves higher accuracy than individual classifiers and based on learning curves, it shows no signs of overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQz98M7FqAkk"
      },
      "outputs": [],
      "source": [
        "# plot classifier accuracy    \n",
        "plt.figure()\n",
        "(_, caps, _) = plt.errorbar(\n",
        "    range(4),\n",
        "    clf_cv_mean,\n",
        "    yerr = clf_cv_std,\n",
        "    c = 'blue',\n",
        "    fmt = '-o',\n",
        "    capsize = 5)\n",
        "\n",
        "for cap in caps:\n",
        "    cap.set_markeredgewidth(1)                                                                                                                                \n",
        "\n",
        "plt.title('Stacking Ensemble')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Classifier')\n",
        "plt.xticks(range(4), ['KNN', 'RF', 'NB', 'Stacking'])     \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS5Tc4z9FoYy"
      },
      "source": [
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxI2We9OFpfs"
      },
      "source": [
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81DoNxN1FqGN"
      },
      "source": [
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RERADKgNFq9T"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> > > > > > > > > © 2022 Institute of Data\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IOD_Demo-8_4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}